{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!activate tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorFlow** is a powerful open source software library for numerical computation, particularly well suited and fine-tuned for large scale Machine Learning. \n",
    "\n",
    "Its basic principle is simple: you first define in python a graph of computations to perform then TensorFlow takes that graph and runs it efficiently using optimized C++ code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Simple Computation Graph](https://pocckg.by3302.livefilestore.com/y4mfrBf3XFd5pBe7r_mAhLfldmThemf6tafYHnVXijt2og6em3rKaoZnnmtcpvlPGQB3g0v5LNesh_6iwVO2yAcFUgNhL7OHDvOAVmj8LB4ebiIZyuRjDFucZRsMiFiCNuVfc66izbBxaRzcyYyGlKgxWB4HxLQCWme5hu8fu-x6MggEfnUVWzeqbvQ20AHxB6RGCXVlzgeMXmXSKj87cKQdg?width=474&height=201&cropmode=none)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most importantly, it is possible to break up the graph into several chunks and run\n",
    "them in parallel across multiple CPUs or GPUs. TensorFlow\n",
    "also supports distributed computing, so you can train colossal neural networks on\n",
    "humongous training sets in a reasonable amount of time by splitting the computa‐\n",
    "tions across hundreds of servers. \n",
    "\n",
    "TensorFlow can train a network\n",
    "with millions of parameters on a training set composed of billions of instances with\n",
    "millions of features each. This should come as no surprise, since TensorFlow was developed by the Google Brain team and it powers many of Google’s large scale services, such as Google Cloud Speech, Google Photos, and Google Search.\n",
    "\n",
    "**Note**\n",
    "1. TF is not limited to neural networks or even Machine Learning, you could run quantum physics simulations\n",
    "if you wanted.\n",
    "2. Not to be confused with the TF Learn library which is an independent project.\n",
    "3. Open Sourced in November 2015.\n",
    "4. Features: Has led it to boost with respect to other libraries like Theano, Caffe, Deepleaning4j, etc..\n",
    "    * Clean Design\n",
    "    * Scalability -- Runs on Linux, Windows, MacOSX and mobile devices\n",
    "    * Flexibility\n",
    "    * Documentation \n",
    "    * Priovides a python API called *TFLearn(tensorflow.contrib.learn)*  compatible with Scikit-Learn \n",
    "    * provides optimization nodes to search for the parameters that minimize a cost function\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://p4cckg.by3302.livefilestore.com/y4mu51nU1CfxMo56gTWbJUHbmgQbJmmP7KWvP-3EjXpTjagQ3eANRWDsXYsWwaSnW9CFjOFOexvX6255jtsczC8G6ZVXdiBguTfZcjYW-MLx2szoWmahJ_cXKuTQ2-qoPJmQlNotP5loPFxS0sJ2j7wnW3X1R_7xnamTyRVbiDaekLqBqIKr2uEzo3KbGC80XEiSojHIh66J5WAS3e-PXjwWw?width=453&height=336&cropmode=none)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating first graph and running it in a session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.Variable(3, name=\"x\")\n",
    "y = tf.Variable(4, name=\"y\")\n",
    "f = x*x*y + y + 2\n",
    "\n",
    "#just creates computation graph, not performs any calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Tensorflow session\n",
    "\n",
    "To evaluate this graph you need to open a TensorFlow session and use it to initialize the variables and evaluate f. \n",
    "\n",
    "A TensorFlow session takes care of placing the operations onto devices such as CPUs and GPUs and running them, and it holds all the variable values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "session = tf.Session()\n",
    "session.run(x.initializer)\n",
    "session.run(y.initializer)\n",
    "result = session.run(f)\n",
    "print(result)\n",
    "session.close() # frees up resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having to repeat sess.run() all the time is a bit cumbersome, but fortunately there is\n",
    "a better way:\n",
    "\n",
    "Inside the with block, the session is set as the default session. \n",
    "Calling x.initial\n",
    "izer.run() is equivalent to calling tf.get_default_session().run(x.initial\n",
    "izer), and\n",
    "similarly f.eval() is equivalent to calling\n",
    "tf.get_default_session().run(f). This makes the code easier to read. Moreover,\n",
    "the session is automatically closed at the end of the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    " x.initializer.run()\n",
    " y.initializer.run()\n",
    " result = f.eval()\n",
    " print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of manually running the initializer for every single variable, you can use the\n",
    "initialize_all_variable() function. Note that it does not actually perform the initialization immediately, it creates a node in the graph that will initialize all variables\n",
    "when it is run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer() #prepare an init node\n",
    "\n",
    "with tf.Session() as session:\n",
    "    init.run() # actual initialization of all the varriables \n",
    "    result = f.eval()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside Jupyter or within a python shell you may prefer to create an **InteractiveSession**. ***The only difference with a regular Session is that when it is created it automatically sets itself as the default session,*** so you don’t need a with block (but you do need\n",
    "to close the session manually when you are done with it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "session = tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)\n",
    "\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Tensorflow program is split into two parts:\n",
    "1. Build Computation Graph (*Construction Phase*)\n",
    "2. Run Computation Graph (*execution Phase*)\n",
    "\n",
    "The construction phase typically builds a computation graph\n",
    "representing the ML model and the computations required to train it. The execution\n",
    "phase generally runs a loop that evaluates a training step repeatedly (for example one\n",
    "step per mini-batch) gradually improving the model parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7776\n"
     ]
    }
   ],
   "source": [
    "x = 2\n",
    "y = 3\n",
    "\n",
    "operation1 = tf.add(x,y)\n",
    "operation2 = tf.multiply(x,y)\n",
    "# it will not get evaluated \n",
    "useless = tf.multiply(x,operation1)\n",
    "\n",
    "operation3 = tf.pow(operation2,operation1)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    z = session.run(operation3)\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we only wants the value of z and z does not depends on useless,\n",
    "**Session will not compute the value of useless**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7776\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "x = 2\n",
    "y = 3\n",
    "\n",
    "operation1 = tf.add(x,y)\n",
    "operation2 = tf.multiply(x,y)\n",
    "# it will not get evaluated \n",
    "useless = tf.multiply(x,operation1)\n",
    "\n",
    "operation3 = tf.pow(operation2,operation1)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    z, not_useless = session.run([operation3,useless])\n",
    "    print(z)\n",
    "    print(not_useless)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pass all variables whose values you wants to a list in fetches "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***It is possible to break graphs into several chunks and run them parallelly across multiple CPUs, Gpus or devies ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Computation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get devices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'CPU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/device:CPU:0']\n"
     ]
    }
   ],
   "source": [
    "print(get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  4.  9. 16. 25. 36.]\n"
     ]
    }
   ],
   "source": [
    "# put part of graph on specific CPU or GPU\n",
    "\n",
    "# Create a graph\n",
    "with tf.device('/CPU:0'):\n",
    "    a = tf.constant([1.0,2.0,3.0,4.0,5.0,6.0],name='a')\n",
    "    b = tf.constant([1.0,2.0,3.0,4.0,5.0,6.0],name='b')\n",
    "    c = tf.multiply(a,b)\n",
    "\n",
    "# create a session with log_device_placement set to true\n",
    "session = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "print(session.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Managing Graphs\n",
    "\n",
    "    Nodes you creates gets added to default graph, But sometimes you may want to manage multiple independent graphs. \n",
    "    You can do it by creating multiple independent graphs. you can do this by creating a new Graph and temporarily \n",
    "    making it default graph inside `with block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    x2 = tf.add(3,5)\n",
    "    \n",
    "print(x2.graph is g)\n",
    "print(x2.graph is tf.get_default_graph())\n",
    "\n",
    "# session = tf.Session()\n",
    "with tf.Session(graph=g) as sess:\n",
    "    print(sess.run(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getting handle to default graph\n",
    "g = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphs best practices\n",
    "    Do not mix default graph and user created graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ERROR PRONE\n",
    "g = tf.Graph()\n",
    "\n",
    "# Add ops to the default graph\n",
    "\n",
    "a = tf.constant(3)\n",
    "# Add ops to user created graph\n",
    "\n",
    "with g.as_default():\n",
    "    b = tf.constant(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# BETTER BUT STILL NOT GOOD ENOUGH because no more than one graph\n",
    "g1 = tf.get_default_graph()\n",
    "g2 = tf.Graph()\n",
    "\n",
    "# add ops to default graph\n",
    "\n",
    "with g1.as_default():\n",
    "    a = tf.constant(3)\n",
    "    \n",
    "# add ops to the user created graph\n",
    "with g1.as_default():\n",
    "    b = tf.constant(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Graphs\n",
    "1. Save computation (only run subgraphs that lead to the values you want to fetch)\n",
    "2. Break computation into small, different pices to facilitates auto-differentiation\n",
    "3. Facilitate distributed computation, spread the work across multiple CPUs, GPUs or devices\n",
    "4. Many common machine learning models are commonly taught and visualized as directed graphs already(NN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lifecycle of Node Value\n",
    "\n",
    "* When you evaluate a node,Tensorflow autometically determines the set of nodes that it depends on and it evaluates these nodes first.For example, consider the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "\n",
    "with tf.Session() as session:\n",
    "    print(y.eval())\n",
    "    print(z.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    First, this code defines a very simple graph. Then it starts a session and runs the graph to evaluate y.\n",
    "    It is important to note that it will not reuse the results from previous evaluation of w and x, \n",
    "    The preceding code evaluates w and x twice.\n",
    "    * A variable start its life when its initializer is run, and it ends when the session is closed.\n",
    "    If you wants to evaluate y and z efficiently, without evaluating w and x twice, you can code like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    y_val,z_val = session.run([y,z])\n",
    "    print(y_val)\n",
    "    print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "**In single-process Tensorflow, multiple session do not share any state, even if they reuse the same graph(each session would have its own copy of every variable). In distributed Tensorflow variable state is stored on the server, not in sessions, so multiple sessions can share the same variables. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. tensor variable vs normal variables "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
